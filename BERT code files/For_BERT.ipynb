{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqr models/official/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_files_path = r'path'\n",
    "\n",
    "covid_files = glob.glob(covid_files_path)\n",
    "train_count = 0\n",
    "test_count = 0\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(covid_files)):\n",
    "    print(\"opening file\", covid_files[i])\n",
    "    while train_count < 25001:\n",
    "        try:\n",
    "            with gzip.open(covid_files[i],'r') as fin:\n",
    "                for line in fin:\n",
    "                    tweet = json.loads(line)\n",
    "                    if train_count <25001:\n",
    "                        if tweet['truncated'] == True:\n",
    "                            train_data.append(tweet['extended_tweet']['full_text'])\n",
    "                        else:\n",
    "                            train_data.append(tweet['text'])\n",
    "                        train_labels.append(1)\n",
    "                        train_count += 1\n",
    "        except:\n",
    "            print(\"Faulty file \", files[i])\n",
    "            \n",
    "    while test_count < 10001:\n",
    "        try:\n",
    "            with gzip.open(covid_files[i],'r') as fin:\n",
    "                for line in fin:\n",
    "                    tweet = json.loads(line)\n",
    "                    if test_count <10001: \n",
    "                        test_data.append()\n",
    "                        test_count += 1\n",
    "        except:\n",
    "            print(\"Faulty file \", files[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncovid_files_path = r'path2'\n",
    "noncovid_files = glob.glob(noncovid_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(noncovid_files)):\n",
    "    print(\"opening file\", noncovid_files[i])\n",
    "    while train_count < 25001:\n",
    "        try:\n",
    "            with gzip.open(noncovid_files[i],'r') as fin:\n",
    "                for line in fin:\n",
    "                    tweet = json.loads(line)\n",
    "                    if train_count <25001: \n",
    "                        train_data.append(tweet['text'])\n",
    "                        train_labels.append(0)\n",
    "                        train_count += 1\n",
    "        except:\n",
    "            print(\"Faulty file \", files[i])\n",
    "            \n",
    "    while test_count < 10001:\n",
    "        try:\n",
    "            with gzip.open(noncovid_files[i],'r') as fin:\n",
    "                for line in fin:\n",
    "                    tweet = json.loads(line)\n",
    "                    if test_count <10001: \n",
    "                        test_data.append()\n",
    "                        test_count += 1\n",
    "        except:\n",
    "            print(\"Faulty file \", files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sys\n",
    "sys.path.append('models')\n",
    "from official.nlp.data import classifier_data_lib\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.Dataframe(train_data, columns = ['Train Data'])\n",
    "test_data_df = pd.Dataframe(test_data, columns = ['Test Data'])\n",
    "train_labels_df = pd.Dataframe(train_data, columns = ['Train Labels'])\n",
    "test_labels_df = pd.Dataframe(train_data, columns = ['Test Labels'])\n",
    "\n",
    "train_data_labels = pd.concat([train_data_df, train_labels_df], axis=1)\n",
    "test_data_labels = pd.concat([test_data_df, test_labels_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, remaining = train_test_split(train_data_labels, random_state = 42, train_size = 0.0075, stratify = df.target.values)\n",
    "valid_df, _ = train_test_split(remaining, random_state = 42, train_size = 0.00075, stratify = remaining.target.values)\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_df['question_text'].values, train_df['target'].values))\n",
    "    valid_data = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values))\n",
    "    \n",
    "    for text, label  in train_data.take(1):\n",
    "        print(text)\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [0, 1] # Label categories\n",
    "max_seq_length = 128 # maximum length of (token) input sequences\n",
    "train_batch_size = 32\n",
    "\n",
    "\n",
    "# Get BERT layer and tokenizer:\n",
    "# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2', trainable = True)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.wordpiece_tokenizer.tokenize(\"hi, how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize(\"hi, how are you doing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n",
    "    example = classifier_data_lib.InputExample(guid = None, text_a = text.numpy(), text_b = None, label = label.numpy())\n",
    "    feature  = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer)\n",
    "    return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_map(text, label):\n",
    "    input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp = [text, label], \n",
    "                                                                Tout = [tf.int32, tf.int32, tf.int32, tf.int32])\n",
    "    input_ids.set_shape([max_seq_length])\n",
    "    input_mask.set_shape([max_seq_length])\n",
    "    segment_ids.set_shape([max_seq_length])\n",
    "    label_id.set_shape([])\n",
    "    x = {\n",
    "      'input_word_ids': input_ids,\n",
    "       'input_mask': input_mask,\n",
    "       'input_type_ids': segment_ids\n",
    "      }\n",
    "    \n",
    "    return (x, label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # train\n",
    "    train_data = (train_data.map(to_feature_map,\n",
    "                               num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "                  .shuffle(1000)\n",
    "                  .batch(32, drop_remainder = True)\n",
    "                  .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "  \n",
    "\n",
    "  # valid\n",
    "    valid_data = (valid_data.map(to_feature_map,\n",
    "                               num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "                  .batch(32, drop_remainder = True)\n",
    "                  .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "def create_model():\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"input_type_ids\")\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "    drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output')(drop)\n",
    "    model = tf.keras.Model(\n",
    "        inputs = {\n",
    "            'input_word_ids': input_word_ids,\n",
    "            'input_mask': input_mask,\n",
    "            'input_type_ids': input_type_ids\n",
    "        },\n",
    "        outputs  = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-5),\n",
    "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "history = model.fit(train_data,\n",
    "                    validation_data = valid_data, \n",
    "                    epochs = epochs,\n",
    "                    verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
