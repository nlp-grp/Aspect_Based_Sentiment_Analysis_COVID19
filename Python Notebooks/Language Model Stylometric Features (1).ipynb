{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "df = pd.read_csv('path_human_annotated_africanamerican_tweets')\n",
    "dff = pd.read_csv('path_human_annotated_africanamerican_tweets2')\n",
    "dfff = pd.read_csv('path_human_annotated_non-africanamerican_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df['race'].to_list() + dff['race'].to_list() + dfff['race'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+' #remove mentions\n",
    "pat2 = r'https?://[^ ]+' #remove hyperlinks\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml') # remove html tags\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") #remove byte order marks\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    lower_case = emoji.demojize(lower_case, language='en')\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled) #remove hashtag\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweet'].to_list() + dff['tweet'].to_list() + dfff['tweet'].to_list()\n",
    "cleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    cleaned_tweets.append(tweet_cleaner_updated(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = []\n",
    "for tweet in cleaned_tweets:\n",
    "    length = 0\n",
    "    for word in tweet.split():\n",
    "        if word != '':\n",
    "            length += 1\n",
    "    num_of_words.append(length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_length = []\n",
    "for tweet in cleaned_tweets:\n",
    "    if tweet != '':\n",
    "        lengths = []\n",
    "        for word in tweet.split():\n",
    "            if word != \" \":\n",
    "                lengths.append(len(word))\n",
    "        avg_word_length.append(sum(lengths)/len(lengths))\n",
    "    else:\n",
    "        avg_word_length.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['races'] = races\n",
    "new_df['tweets'] = cleaned_tweets\n",
    "new_df['num_of_words'] = num_of_words\n",
    "new_df['avg_word_length'] = avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = new_df[new_df['races'] == 0]\n",
    "positive_tweets = new_df[new_df['races'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets_num_of_words = negative_tweets['num_of_words'].to_list()\n",
    "pos_tweets_num_of_words = positive_tweets['num_of_words'].to_list()\n",
    "len(neg_tweets_num_of_words)\n",
    "x_pos = [i for i in range(3411)]\n",
    "x_neg = [i for i in range(3411)]\n",
    "plt.scatter(x_pos, pos_tweets_num_of_words,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(x_neg, neg_tweets_num_of_words, marker='o',  color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets_avg_word_len = negative_tweets['avg_word_length'].to_list()\n",
    "pos_tweets_avg_word_len = positive_tweets['avg_word_length'].to_list()\n",
    "len(neg_tweets_num_of_words)\n",
    "x_pos = [i for i in range(3411)]\n",
    "x_neg = [i for i in range(3411)]\n",
    "plt.scatter(x_pos, pos_tweets_avg_word_len,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(x_neg, neg_tweets_avg_word_len, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(pos_tweets_num_of_words, pos_tweets_avg_word_len,  marker='^', color = 'purple')\n",
    "plt.scatter(neg_tweets_num_of_words, neg_tweets_avg_word_len, marker='o', color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_words = pd.DataFrame()\n",
    "negs = negative_tweets['num_of_words'].to_list()\n",
    "pos = positive_tweets['num_of_words'].to_list()\n",
    "df_num_words['neg_num_of_words'] = negs\n",
    "df_num_words['pos_num_of_words'] = pos\n",
    "df_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_num_of_words\", y=\"pos_num_of_words\",fit_reg=False, scatter_kws={'alpha':0.5}, data=df_num_words)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_wordlen = pd.DataFrame()\n",
    "negs = negative_tweets['avg_word_length'].to_list()\n",
    "pos = positive_tweets['avg_word_length'].to_list()\n",
    "df_avg_wordlen['neg_avg_word_length'] = negs\n",
    "df_avg_wordlen['pos_avg_word_length'] = pos\n",
    "df_avg_wordlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_avg_word_length\", y=\"pos_avg_word_length\",fit_reg=False, scatter_kws={'alpha':0.5}, data=df_avg_wordlen)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+' #remove mentions\n",
    "pat2 = r'https?://[^ ]+' #remove hyperlinks\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "\n",
    "def tweet_cleaner_updated2(text):\n",
    "    soup = BeautifulSoup(text, 'lxml') # remove html tags\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") #remove byte order marks\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    uncleaned_tweets.append(tweet_cleaner_updated2(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_tweets[3]\n",
    "print(re.split(\" |\\n\",uncleaned_tweets[3])[0] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertools.emoji import extract_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncleaned_num_of_words = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    length = 0\n",
    "    for word in re.split(\" |\\n\",tweet):\n",
    "        if word != '':\n",
    "            length += 1\n",
    "    uncleaned_num_of_words.append(length)\n",
    "    \n",
    "uncleaned_avg_word_length = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    if tweet != '':\n",
    "        lengths = []\n",
    "        for word in re.split(\" |\\n\",tweet):\n",
    "            if word != \" \":\n",
    "                lengths.append(len(word))\n",
    "        uncleaned_avg_word_length.append(sum(lengths)/len(lengths))\n",
    "    else:\n",
    "        uncleaned_avg_word_length.append(0)\n",
    "        \n",
    "        \n",
    "uncleaned_count_emoji = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    count = 0\n",
    "    for i in range(len(extract_emoji(tweet)['emoji_flat_text'])):\n",
    "        count += 1\n",
    "    uncleaned_count_emoji.append(count)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punctuation = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "uncleaned_count_puncts = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    uncleaned_count_puncts.append(count_punctuation(tweet,set(string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncleaned_new_df = pd.DataFrame()\n",
    "uncleaned_new_df['races'] = races\n",
    "uncleaned_new_df['tweets'] = uncleaned_tweets\n",
    "uncleaned_new_df['num_of_words'] = uncleaned_num_of_words\n",
    "uncleaned_new_df['avg_word_length'] = uncleaned_avg_word_length\n",
    "uncleaned_new_df['count_emoji'] = uncleaned_count_emoji\n",
    "uncleaned_new_df['count_puncts'] = uncleaned_count_puncts\n",
    "\n",
    "uncleaned_negative_tweets = uncleaned_new_df[uncleaned_new_df['races'] == 0]\n",
    "uncleaned_positive_tweets = uncleaned_new_df[uncleaned_new_df['races'] == 1]\n",
    "\n",
    "uncleaned_neg_tweets_num_of_words = uncleaned_negative_tweets['num_of_words'].to_list()\n",
    "uncleaned_pos_tweets_num_of_words = uncleaned_positive_tweets['num_of_words'].to_list()\n",
    "len(uncleaned_neg_tweets_num_of_words)\n",
    "uncleaned_x_pos = [i for i in range(3411)]\n",
    "uncleaned_x_neg = [i for i in range(3411)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos_tweets_num_of_words,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_neg_tweets_num_of_words, marker='o',  color = 'green')\n",
    "plt.show()\n",
    "\n",
    "uncleaned_neg_tweets_avg_word_len = uncleaned_negative_tweets['avg_word_length'].to_list()\n",
    "uncleaned_pos_tweets_avg_word_len = uncleaned_positive_tweets['avg_word_length'].to_list()\n",
    "len(uncleaned_neg_tweets_num_of_words)\n",
    "uncleaned_x_pos = [i for i in range(3411)]\n",
    "uncleaned_x_neg = [i for i in range(3411)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos_tweets_avg_word_len,  marker='^', color = 'purple')\n",
    "#plt.scatter(x_neg, neg_tweets_num_of_words, marker='o')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_neg_tweets_avg_word_len, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "uncleaned_df_emoji= pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['count_emoji'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['count_emoji'].to_list()\n",
    "uncleaned_df_emoji['neg_count_emoji'] = uncleaned_negs\n",
    "uncleaned_df_emoji['pos_count_emoji'] = uncleaned_pos\n",
    "uncleaned_df_emoji\n",
    "\n",
    "\n",
    "\n",
    "uncleaned_x_pos = [i for i in range(3411)]\n",
    "uncleaned_x_neg = [i for i in range(3411)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos,marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_negs, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "uncleaned_df_num_words = pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['num_of_words'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['num_of_words'].to_list()\n",
    "uncleaned_df_num_words['neg_num_of_words'] = uncleaned_negs\n",
    "uncleaned_df_num_words['pos_num_of_words'] = uncleaned_pos\n",
    "uncleaned_df_num_words\n",
    "\n",
    "\n",
    "uncleaned_df_avg_wordlen = pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['avg_word_length'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['avg_word_length'].to_list()\n",
    "uncleaned_df_avg_wordlen['neg_avg_word_length'] = uncleaned_negs\n",
    "uncleaned_df_avg_wordlen['pos_avg_word_length'] = uncleaned_pos\n",
    "uncleaned_df_avg_wordlen\n",
    "\n",
    "\n",
    "plt.scatter(uncleaned_pos_tweets_avg_word_len, uncleaned_pos_tweets_num_of_words,  marker='^', color = 'purple')\n",
    "plt.scatter(uncleaned_neg_tweets_avg_word_len, uncleaned_neg_tweets_num_of_words, marker='o',  color = 'green')\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_df_puncts= pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['count_puncts'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['count_puncts'].to_list()\n",
    "uncleaned_df_puncts['neg_count_puncts'] = uncleaned_negs\n",
    "uncleaned_df_puncts['pos_count_puncts'] = uncleaned_pos\n",
    "\n",
    "\n",
    "uncleaned_x_pos = [i for i in range(3411)]\n",
    "uncleaned_x_neg = [i for i in range(3411)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos,marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_negs, marker='o', color = 'green')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "uncleaned_negss = uncleaned_negative_tweets['count_emoji'].to_list()\n",
    "uncleaned_poss = uncleaned_positive_tweets['count_emoji'].to_list()\n",
    "\n",
    "plt.scatter(uncleaned_poss, uncleaned_pos,marker='^', color = 'purple')\n",
    "plt.scatter(uncleaned_negss, uncleaned_negs, marker='o', color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_count_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in cleaned_tweets:\n",
    "    wordsList = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if w not in stop_words] \n",
    "\n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)\n",
    "    counts = Counter( tag for word,  tag in tagged)\n",
    "    \n",
    "    print(counts)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_pos_map = {v: k for k, v in  pos_code_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abbrivation converters\n",
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tag converting\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = train_test_split(cleaned_tweets,races,stratify=races,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_for_tweets = []\n",
    "for tweet in train_corpus:\n",
    "    text_pos = lambda x: pos_tags(tweet)\n",
    "    pos_for_tweets.append(text_pos(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_for_tweets_test = []\n",
    "for tweet in test_corpus:\n",
    "    text_pos = lambda x: pos_tags(tweet)\n",
    "    pos_for_tweets_test.append(text_pos(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = TfidfVectorizer(stop_words = 'english', analyzer='char',ngram_range=(1, 1))\n",
    "ngrams = v.fit_transform(pos_for_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_val = v.transform(pos_for_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf2 = SVC(random_state=0).fit(ngrams, train_labels)\n",
    "\n",
    "y_pred = clf2.predict(ngrams_val)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TfidfVectorizer(stop_words = 'english',max_features=10000,ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = t.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_val = t.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "train_train = hstack((tfidf_train,ngrams))\n",
    "test_test = hstack((tfidf_val,ngrams_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf2 = SVC(random_state=0).fit(train_train, train_labels)\n",
    "\n",
    "y_pred = clf2.predict(test_test)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = []\n",
    "for abbrv_pos in pos_for_tweets:\n",
    "    funct = lambda x: text_pos_inv_convert(abbrv_pos)\n",
    "    pos_train.append(funct(abbrv_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = []\n",
    "for tweet in train_corpus:\n",
    "    wordsList = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if w not in stop_words] \n",
    "\n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    counts = Counter( tag for word,  tag in tagged)\n",
    "    \n",
    "    pos_vector = []\n",
    "    for tag in pos_list:\n",
    "        if tag in counts.keys():\n",
    "            pos_vector.append(counts[tag])\n",
    "        else:\n",
    "            pos_vector.append(0)\n",
    "    \n",
    "    normalized_pos = []\n",
    "    summ = sum(pos_vector)\n",
    "    for vec in pos_vector:\n",
    "        if summ != 0:\n",
    "            normalized_pos.append(vec/summ)\n",
    "        else:\n",
    "            normalized_pos.append(summ)\n",
    "    #pos_train.append(pos_vector)\n",
    "    pos_train.append(normalized_pos)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_val = []\n",
    "for tweet in test_corpus:\n",
    "    wordsList = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if w not in stop_words] \n",
    "\n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    counts = Counter( tag for word,  tag in tagged)\n",
    "    \n",
    "    pos_vector = []\n",
    "    for tag in pos_list:\n",
    "        if tag in counts.keys():\n",
    "            pos_vector.append(counts[tag])\n",
    "        else:\n",
    "            pos_vector.append(0)\n",
    "    #print(pos_vector)\n",
    "    \n",
    "    normalized_pos = []\n",
    "    summ = sum(pos_vector)\n",
    "    for vec in pos_vector:\n",
    "        if summ != 0:\n",
    "            normalized_pos.append(vec/summ)\n",
    "        else:\n",
    "            normalized_pos.append(summ)\n",
    "    pos_val.append(normalized_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression(max_iter=200,random_state=0).fit(pos_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf1.predict(pos_val)\n",
    "accuracy = accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf2 = SVC(random_state=0).fit(pos_train, train_labels)\n",
    "\n",
    "y_pred = clf2.predict(pos_val)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf3 = MultinomialNB().fit(pos_train, train_labels)\n",
    "\n",
    "y_pred = clf3.predict(pos_val)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
