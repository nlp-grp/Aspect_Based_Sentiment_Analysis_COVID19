{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "df=pd.read_csv('survey_data')\n",
    "responses=df.iloc[2:,12:]\n",
    "\n",
    "reordered_responses=responses[['Q34','Q58','Q12','Q13','Q14','Q16','Q35','Q18','Q19','Q20','Q21','Q30','Q34','Q17','Q15','Q1','Q39','Q28','Q3','Q4','Q5','Q6','Q9','Q29_1','Q29_2','Q29_3','Q29_4','Q29_5','Q29_6','Q29_7']]\n",
    "\n",
    "free_form_responses=reordered_responses.iloc[:,:12]\n",
    "\n",
    "single_choice_responses=reordered_responses.iloc[:,12:]\n",
    "\n",
    "individual_responses=collections.defaultdict(list)\n",
    "free_form_responses_without_race=free_form_responses.iloc[:,1:]\n",
    "race=free_form_responses.iloc[:,0].tolist()\n",
    "r=-1\n",
    "for index, row in free_form_responses_without_race.iterrows():\n",
    "    r+=1\n",
    "    for i in range(len(row)):\n",
    "        try:\n",
    "            if float(row[i])==row[i]:\n",
    "                continue\n",
    "        except ValueError:\n",
    "            individual_responses[(index,race[r])].append(row[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(individual_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_raw = []\n",
    "survey_labels = []\n",
    "for i in individual_responses:\n",
    "    for z in individual_responses[i]:\n",
    "        survey_data_raw.append(z)\n",
    "        if i[1] == 'Black or African American':\n",
    "            survey_labels.append(1)\n",
    "        else:\n",
    "            survey_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_box_plot = [len(t) for t in survey_data_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(survey_box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survey_data_raw[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(survey_data_raw))\n",
    "print(len(set(survey_data_raw)))\n",
    "print(len(survey_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df1 = pd.DataFrame(columns=['text','label'])\n",
    "df1['text'] =  survey_data_raw\n",
    "df1['label'] = survey_labels\n",
    "df1['pre_clean_len'] = [len(t) for t in df1.text]\n",
    "df1[df1.pre_clean_len > 280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for data in survey_data_raw:\n",
    "    train_data.append(tweet_cleaner_updated(data))\n",
    "    \n",
    "train_labels = survey_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_box_plot2 = [len(t) for t in train_data]\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(survey_box_plot2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['text','label'])\n",
    "\n",
    "df['text'] = train_data\n",
    "df['label'] = train_labels\n",
    "\n",
    "df0 = df[df[\"label\"] == 0] \n",
    "df1 = df[df[\"label\"] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = df[df.label == 0]\n",
    "neg_string = []\n",
    "for t in neg_tweets.text:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = df[df.label == 1]\n",
    "pos_string = []\n",
    "for t in pos_tweets.text:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \n",
    "plt.figure(figsize=(12,10)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = cvec.transform(df[df.label == 0].text)\n",
    "pos_doc_matrix = cvec.transform(df[df.label == 1].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english',max_features=3000)\n",
    "cvec.fit(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = cvec.transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix2 = cvec.transform(df[df.label == 0].text)\n",
    "pos_doc_matrix2 = cvec.transform(df[df.label == 1].text)\n",
    "neg_tf2 = np.sum(neg_doc_matrix2,axis=0)\n",
    "pos_tf2 = np.sum(pos_doc_matrix2,axis=0)\n",
    "neg2 = np.squeeze(np.asarray(neg_tf2))\n",
    "pos2 = np.squeeze(np.asarray(pos_tf2))\n",
    "term_freq_df2 = pd.DataFrame([neg2,pos2],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negative', 'positive']\n",
    "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive']\n",
    "term_freq_df2.sort_values(by='total', ascending=False).iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"negative\", y=\"positive\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
    "term_freq_df2.sort_values(by='pos_rate', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_rate'] = term_freq_df2['negative'] * 1./term_freq_df2['total']\n",
    "term_freq_df2.sort_values(by='neg_rate', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
    "term_freq_df2.sort_values(by='pos_freq_pct', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_freq_pct'] = term_freq_df2['negative'] * 1./term_freq_df2['negative'].sum()\n",
    "term_freq_df2.sort_values(by='neg_freq_pct', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean\n",
    "\n",
    "term_freq_df2['pos_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['pos_rate'], x['pos_freq_pct']])\n",
    "                                                                   if x['pos_rate'] > 0 and x['pos_freq_pct'] > 0 \n",
    "                                                                   else 0), axis=1)                                                        \n",
    "term_freq_df2.sort_values(by='pos_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['neg_rate'], x['neg_freq_pct']])\n",
    "                                                                   if x['neg_rate'] > 0 and x['neg_freq_pct'] > 0 \n",
    "                                                                   else 0), axis=1)                                                        \n",
    "term_freq_df2.sort_values(by='neg_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
    "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
    "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='pos_normcdf_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "term_freq_df2['neg_rate_normcdf'] = normcdf(term_freq_df2['neg_rate'])\n",
    "term_freq_df2['neg_freq_pct_normcdf'] = normcdf(term_freq_df2['neg_freq_pct'])\n",
    "term_freq_df2['neg_normcdf_hmean'] = hmean([term_freq_df2['neg_rate_normcdf'], term_freq_df2['neg_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='neg_normcdf_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_freq_pct\", y=\"pos_freq_pct\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Frequency Rate')\n",
    "plt.xlabel('Negative Frequency Rate')\n",
    "plt.title('neg_freq_pct vs pos_freq_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_hmean\", y=\"pos_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Rate and Frequency Harmonic Mean')\n",
    "plt.xlabel('Negative Rate and Frequency Harmonic Mean')\n",
    "plt.title('neg_hmean vs pos_hmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_normcdf_hmean\", y=\"pos_normcdf_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Rate and Frequency CDF Harmonic Mean')\n",
    "plt.xlabel('Negative Rate and Frequency CDF Harmonic Mean')\n",
    "plt.title('neg_normcdf_hmean vs pos_normcdf_hmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import LinearColorMapper\n",
    "output_notebook()\n",
    "from bokeh.resources import INLINE\n",
    "import bokeh.io\n",
    "\n",
    "\n",
    "color_mapper = LinearColorMapper(palette='Inferno256', low=min(term_freq_df2.pos_normcdf_hmean), high=max(term_freq_df2.pos_normcdf_hmean))\n",
    "p = figure(x_axis_label='neg_normcdf_hmean', y_axis_label='pos_normcdf_hmean')\n",
    "p.circle('neg_normcdf_hmean','pos_normcdf_hmean',size=5,alpha=0.3,source=term_freq_df2,color={'field': 'pos_normcdf_hmean', 'transform': color_mapper})\n",
    "from bokeh.models import HoverTool\n",
    "hover = HoverTool(tooltips=[('token','@index')])\n",
    "p.add_tools(hover)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = train_test_split(train_data,train_labels,stratify=train_labels,test_size=0.25,random_state=1)\n",
    "train_texts, train_labels, test_texts, test_labels = train_corpus, train_labels, test_corpus, test_labels\n",
    "x_train, y_train, x_validation, y_validation = train_corpus,train_labels,test_corpus,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_len0 = 0\n",
    "y_train_len1 = 1\n",
    "for label in y_train:\n",
    "    if label == 1:\n",
    "        y_train_len1 += 1\n",
    "    else:\n",
    "        y_train_len0 += 1\n",
    "        \n",
    "y_val_len0 = 0\n",
    "y_val_len1 = 0\n",
    "for label in y_validation:\n",
    "    if label == 1:\n",
    "        y_val_len1 += 1\n",
    "    else:\n",
    "        y_val_len0 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (y_train_len0 / (len(x_train)*1.))*100,\n",
    "                                                                            (y_train_len1 / (len(x_train)*1.))*100))\n",
    "       \n",
    "                                                                            \n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (y_val_len0 / (len(x_validation)))*100,\n",
    "                                                                            (y_val_len1 / (len(x_validation)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    #t0 = time.time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    #train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    #print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "n_features = np.arange(1000,4001,1000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "z = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:3].index))\n",
    "c = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[4:11].index))\n",
    "listofsets = [z,c]\n",
    "a = frozenset.union(*listofsets)\n",
    "b = text.ENGLISH_STOP_WORDS\n",
    "set(a).issubset(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
    "feature_result_wosw = nfeature_accuracy_checker(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_ug = nfeature_accuracy_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\")\n",
    "feature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_bg = nfeature_accuracy_checker(stop_words=my_stop_words,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_bg2 = nfeature_accuracy_checker(stop_words='english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg = nfeature_accuracy_checker(stop_words=my_stop_words, ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg2 = nfeature_accuracy_checker(stop_words='english', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tg2 = pd.DataFrame(feature_result_tg2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg2 = pd.DataFrame(feature_result_bg2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug2 = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram without custom sw')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram without custom sw')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram without custom sw')\n",
    "plt.plot(nfeatures_plot_tg2.nfeatures, nfeatures_plot_tg2.validation_accuracy,label='trigram without sw')\n",
    "plt.plot(nfeatures_plot_bg2.nfeatures, nfeatures_plot_bg2.validation_accuracy,label='bigram without sw')\n",
    "plt.plot(nfeatures_plot_ug2.nfeatures, nfeatures_plot_ug2.validation_accuracy, label='unigram without sw')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "    confusion = pd.DataFrame(conmat, index=['non_covid', 'covid'],\n",
    "                         columns=['predicted_non_covid','predicted_covid'])\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print (\"-\"*80)\n",
    "    print (\"Confusion Matrix\\n\")\n",
    "    print (confusion)\n",
    "    print (\"-\"*80)\n",
    "    print (\"Classification Report\\n\")\n",
    "    print (classification_report(y_test, y_pred, target_names=['negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tg_cvec = CountVectorizer(max_features=4000,ngram_range=(1, 3))\n",
    "tg_pipeline = Pipeline([\n",
    "        ('vectorizer', tg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt3 = nfeature_accuracy_checker(vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITHOUT CUSTOM STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt3 = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT CUSTOM  STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt3 = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt2 = pd.DataFrame(feature_result_ugt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt3 = pd.DataFrame(feature_result_ugt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ugt2.nfeatures, nfeatures_plot_ugt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_ugt3.nfeatures, nfeatures_plot_ugt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt2 = pd.DataFrame(feature_result_bgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt3 = pd.DataFrame(feature_result_bgt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_bgt2.nfeatures, nfeatures_plot_bgt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_bgt3.nfeatures, nfeatures_plot_bgt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Bigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tgt2 = pd.DataFrame(feature_result_tgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tgt3 = pd.DataFrame(feature_result_tgt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_tgt2.nfeatures, nfeatures_plot_tgt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_tgt3.nfeatures, nfeatures_plot_tgt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Trigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt2 = pd.DataFrame(feature_result_tgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt2 = pd.DataFrame(feature_result_bgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt2 = pd.DataFrame(feature_result_ugt2,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt2.nfeatures, nfeatures_plot_tgt2.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg2.nfeatures, nfeatures_plot_tg2.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt2.nfeatures, nfeatures_plot_bgt2.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg2.nfeatures, nfeatures_plot_bg2.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt2.nfeatures, nfeatures_plot_ugt2.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\" Without using stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\"Without using custom stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_result_bg3 = nfeature_accuracy_checker(ngram_range=(1, 2))\n",
    "feature_result_tg3 = nfeature_accuracy_checker(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt3 = pd.DataFrame(feature_result_tgt3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt3 = pd.DataFrame(feature_result_bgt3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt3= pd.DataFrame(feature_result_ugt3,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "nfeatures_plot_tg3 = pd.DataFrame(feature_result_tg3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg3 = pd.DataFrame(feature_result_bg3,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt3.nfeatures, nfeatures_plot_tgt3.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg3.nfeatures, nfeatures_plot_tg3.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt3.nfeatures, nfeatures_plot_bgt3.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg3.nfeatures, nfeatures_plot_bg3.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt3.nfeatures, nfeatures_plot_ugt3.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\"Using stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary2(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    #t0 = time.time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    #train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    #print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy, sentiment_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=2000,ngram_range=(1, 3))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy, model = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
