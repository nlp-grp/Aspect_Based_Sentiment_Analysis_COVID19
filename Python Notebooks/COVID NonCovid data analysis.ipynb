{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_files_path = r'path'\n",
    "\n",
    "covid_train_data = []\n",
    "noncovid_train_data = []\n",
    "covid_train_labels = []\n",
    "noncovid_train_labels = []\n",
    "\n",
    "covid_train_data2 = []\n",
    "noncovid_train_data2 = []\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(covid_files_path,'r') as fin:\n",
    "        for line in fin:\n",
    "            tweet = json.loads(line)\n",
    "            covid_train_data.append(tweet['text'])\n",
    "            covid_train_labels.append(1)\n",
    "except:\n",
    "    print(\"Faulty file \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_box_plot = [len(t) for t in covid_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(covid_box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncovid_files_path = r'path2'\n",
    "noncovid_files = glob.glob(noncovid_files_path)\n",
    "num_noncovid_tweets = 0\n",
    "\n",
    "for i in range(len(noncovid_files)):\n",
    "    print(\"opening file\", noncovid_files[i])\n",
    "    try:\n",
    "        with gzip.open(noncovid_files[i],'r') as fin:\n",
    "            for line in fin:\n",
    "                if num_noncovid_tweets < 27068:\n",
    "                    tweet = json.loads(line)\n",
    "                    noncovid_train_data.append(tweet['text'])\n",
    "                    noncovid_train_labels.append(0)\n",
    "                    num_noncovid_tweets += 1\n",
    "\n",
    "    except:\n",
    "        print(\"Faulty file \", noncovid_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncovid_box_plot = [len(t) for t in noncovid_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df1 = pd.DataFrame(columns=['text','label'])\n",
    "df1['text'] = covid_train_data\n",
    "df1['label'] = covid_train_labels\n",
    "df1['pre_clean_len'] = [len(t) for t in df1.text]\n",
    "df1[df1.pre_clean_len > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(noncovid_box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df2 = pd.DataFrame(columns=['text','label'])\n",
    "df2['text'] = noncovid_train_data\n",
    "df2['label'] = noncovid_train_labels\n",
    "df2['pre_clean_len'] = [len(t) for t in df2.text]\n",
    "df2[df2.pre_clean_len > 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in covid_train_data:\n",
    "    covid_train_data2.append(tweet_cleaner_updated(tweet))\n",
    "    \n",
    "for ntweet in noncovid_train_data:\n",
    "    noncovid_train_data2.append(tweet_cleaner_updated(ntweet))\n",
    "\n",
    "train_data = covid_train_data2 + noncovid_train_data2\n",
    "train_labels = covid_train_labels + noncovid_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_box_plot2 = [len(t) for t in covid_train_data2]\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(covid_box_plot2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncovid_box_plot2 = [len(t) for t in noncovid_train_data2]\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(noncovid_box_plot2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df3 = pd.DataFrame(columns=['text','label'])\n",
    "df3['text'] = noncovid_train_data2\n",
    "df3['label'] = noncovid_train_labels\n",
    "df3['pre_clean_len'] = [len(t) for t in df3.text]\n",
    "df3[df3.pre_clean_len > 150].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df4 = pd.DataFrame(columns=['text','label'])\n",
    "df4['text'] = covid_train_data2\n",
    "df4['label'] = covid_train_labels\n",
    "df4['pre_clean_len'] = [len(t) for t in df4.text]\n",
    "df4[df4.pre_clean_len > 150].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = df3[df3.label == 0]\n",
    "neg_string = []\n",
    "for t in neg_tweets.text:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = df4[df4.label == 1]\n",
    "pos_string = []\n",
    "for t in pos_tweets.text:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \n",
    "plt.figure(figsize=(12,10)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in neg_tweets.text[:200]:\n",
    "    if 'china' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in pos_tweets.text[:800]:\n",
    "    if 'china' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = covid_train_data2 + noncovid_train_data2\n",
    "train_labels = covid_train_labels + noncovid_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(columns=['text','target'])\n",
    "my_df['text'] = train_data\n",
    "my_df['target'] = train_labels\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(my_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)\n",
    "pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_doc_matrix[1])\n",
    "print(neg[:20])\n",
    "print(neg_doc_matrix.toarray()[10])\n",
    "print(term_freq_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english',max_features=10000)\n",
    "cvec.fit(my_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = cvec.transform(my_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix2 = cvec.transform(my_df[my_df.target == 0].text)\n",
    "pos_doc_matrix2 = cvec.transform(my_df[my_df.target == 1].text)\n",
    "neg_tf2 = np.sum(neg_doc_matrix2,axis=0)\n",
    "pos_tf2 = np.sum(pos_doc_matrix2,axis=0)\n",
    "neg2 = np.squeeze(np.asarray(neg_tf2))\n",
    "pos2 = np.squeeze(np.asarray(pos_tf2))\n",
    "term_freq_df2 = pd.DataFrame([neg2,pos2],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negative', 'positive']\n",
    "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive']\n",
    "term_freq_df2.sort_values(by='total', ascending=False).iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"negative\", y=\"positive\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
    "term_freq_df2.sort_values(by='pos_rate', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_rate'] = term_freq_df2['negative'] * 1./term_freq_df2['total']\n",
    "term_freq_df2.sort_values(by='neg_rate', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
    "term_freq_df2.sort_values(by='pos_freq_pct', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_freq_pct'] = term_freq_df2['negative'] * 1./term_freq_df2['negative'].sum()\n",
    "term_freq_df2.sort_values(by='neg_freq_pct', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean\n",
    "\n",
    "term_freq_df2['pos_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['pos_rate'], x['pos_freq_pct']])\n",
    "                                                                   if x['pos_rate'] > 0 and x['pos_freq_pct'] > 0 \n",
    "                                                                   else 0), axis=1)                                                        \n",
    "term_freq_df2.sort_values(by='pos_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2['neg_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['neg_rate'], x['neg_freq_pct']])\n",
    "                                                                   if x['neg_rate'] > 0 and x['neg_freq_pct'] > 0 \n",
    "                                                                   else 0), axis=1)                                                        \n",
    "term_freq_df2.sort_values(by='neg_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
    "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
    "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='pos_normcdf_hmean', ascending=False).iloc[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "term_freq_df2['neg_rate_normcdf'] = normcdf(term_freq_df2['neg_rate'])\n",
    "term_freq_df2['neg_freq_pct_normcdf'] = normcdf(term_freq_df2['neg_freq_pct'])\n",
    "term_freq_df2['neg_normcdf_hmean'] = hmean([term_freq_df2['neg_rate_normcdf'], term_freq_df2['neg_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='neg_normcdf_hmean', ascending=False).iloc[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_freq_pct\", y=\"pos_freq_pct\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Frequency Rate')\n",
    "plt.xlabel('Negative Frequency Rate')\n",
    "plt.title('neg_freq_pct vs pos_freq_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_hmean\", y=\"pos_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Rate and Frequency Harmonic Mean')\n",
    "plt.xlabel('Negative Rate and Frequency Harmonic Mean')\n",
    "plt.title('neg_hmean vs pos_hmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_normcdf_hmean\", y=\"pos_normcdf_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
    "plt.ylabel('Positive Rate and Frequency CDF Harmonic Mean')\n",
    "plt.xlabel('Negative Rate and Frequency CDF Harmonic Mean')\n",
    "plt.title('neg_normcdf_hmean vs pos_normcdf_hmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import LinearColorMapper\n",
    "output_notebook()\n",
    "from bokeh.resources import INLINE\n",
    "import bokeh.io\n",
    "\n",
    "\n",
    "color_mapper = LinearColorMapper(palette='Inferno256', low=min(term_freq_df2.pos_normcdf_hmean), high=max(term_freq_df2.pos_normcdf_hmean))\n",
    "p = figure(x_axis_label='neg_normcdf_hmean', y_axis_label='pos_normcdf_hmean')\n",
    "p.circle('neg_normcdf_hmean','pos_normcdf_hmean',size=5,alpha=0.3,source=term_freq_df2,color={'field': 'pos_normcdf_hmean', 'transform': color_mapper})\n",
    "from bokeh.models import HoverTool\n",
    "hover = HoverTool(tooltips=[('token','@index')])\n",
    "p.add_tools(hover)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = train_test_split(train_data,train_labels,stratify=train_labels,test_size=0.25,random_state=1)\n",
    "train_texts, train_labels, test_texts, test_labels = train_corpus, train_labels, test_corpus, test_labels\n",
    "x_train, y_train, x_validation, y_validation = train_corpus,train_labels,test_corpus,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation = train_corpus,train_labels,test_corpus,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_len0 = 0\n",
    "y_train_len1 = 1\n",
    "for label in y_train:\n",
    "    if label == 1:\n",
    "        y_train_len1 += 1\n",
    "    else:\n",
    "        y_train_len0 += 1\n",
    "        \n",
    "y_val_len0 = 0\n",
    "y_val_len1 = 0\n",
    "for label in y_validation:\n",
    "    if label == 1:\n",
    "        y_val_len1 += 1\n",
    "    else:\n",
    "        y_val_len0 += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (y_train_len0 / (len(x_train)*1.))*100,\n",
    "                                                                            (y_train_len1 / (len(x_train)*1.))*100))\n",
    "       \n",
    "                                                                            \n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (y_val_len0 / (len(x_validation)))*100,\n",
    "                                                                            (y_val_len1 / (len(x_validation)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    #t0 = time.time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    #train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    #print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "n_features = np.arange(10000,48001,10000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "z = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:3].index))\n",
    "c = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[4:11].index))\n",
    "listofsets = [z,c]\n",
    "a = frozenset.union(*listofsets)\n",
    "b = text.ENGLISH_STOP_WORDS\n",
    "set(a).issubset(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
    "feature_result_wosw = nfeature_accuracy_checker(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_ug = nfeature_accuracy_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\")\n",
    "feature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_bg = nfeature_accuracy_checker(stop_words=my_stop_words,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_bg2 = nfeature_accuracy_checker(stop_words='english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg = nfeature_accuracy_checker(stop_words=my_stop_words, ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
    "feature_result_tg2 = nfeature_accuracy_checker(stop_words='english', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tg2 = pd.DataFrame(feature_result_tg2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg2 = pd.DataFrame(feature_result_bg2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ug2 = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram without custom sw')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram without custom sw')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram without custom sw')\n",
    "plt.plot(nfeatures_plot_tg2.nfeatures, nfeatures_plot_tg2.validation_accuracy,label='trigram without sw')\n",
    "plt.plot(nfeatures_plot_bg2.nfeatures, nfeatures_plot_bg2.validation_accuracy,label='bigram without sw')\n",
    "plt.plot(nfeatures_plot_ug2.nfeatures, nfeatures_plot_ug2.validation_accuracy, label='unigram without sw')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "    confusion = pd.DataFrame(conmat, index=['non_covid', 'covid'],\n",
    "                         columns=['predicted_non_covid','predicted_covid'])\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print (\"-\"*80)\n",
    "    print (\"Confusion Matrix\\n\")\n",
    "    print (confusion)\n",
    "    print (\"-\"*80)\n",
    "    print (\"Classification Report\\n\")\n",
    "    print (classification_report(y_test, y_pred, target_names=['negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 3))\n",
    "tg_pipeline = Pipeline([\n",
    "        ('vectorizer', tg_cvec),\n",
    "        ('classifier', lr)\n",
    "    ])\n",
    "train_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR UNIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_ugt3 = nfeature_accuracy_checker(vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITHOUT CUSTOM STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_bgt3 = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT CUSTOM  STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt = nfeature_accuracy_checker(stop_words = my_stop_words,vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt2 = nfeature_accuracy_checker(stop_words = 'english',vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt3 = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt2 = pd.DataFrame(feature_result_ugt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt3 = pd.DataFrame(feature_result_ugt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_ugt2.nfeatures, nfeatures_plot_ugt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_ugt3.nfeatures, nfeatures_plot_ugt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt2 = pd.DataFrame(feature_result_bgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt3 = pd.DataFrame(feature_result_bgt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_bgt2.nfeatures, nfeatures_plot_bgt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_bgt3.nfeatures, nfeatures_plot_bgt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Bigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tgt2 = pd.DataFrame(feature_result_tgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_tgt3 = pd.DataFrame(feature_result_tgt3,columns=['nfeatures','validation_accuracy'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy, label='without custom stop words')\n",
    "plt.plot(nfeatures_plot_tgt2.nfeatures, nfeatures_plot_tgt2.validation_accuracy,label='without stop words')\n",
    "plt.plot(nfeatures_plot_tgt3.nfeatures, nfeatures_plot_tgt3.validation_accuracy,label='with stop words')\n",
    "plt.title(\" tfidf Without stop words VS With stop words (Trigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt2 = pd.DataFrame(feature_result_tgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt2 = pd.DataFrame(feature_result_bgt2,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt2 = pd.DataFrame(feature_result_ugt2,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt2.nfeatures, nfeatures_plot_tgt2.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg2.nfeatures, nfeatures_plot_tg2.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt2.nfeatures, nfeatures_plot_bgt2.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg2.nfeatures, nfeatures_plot_bg2.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt2.nfeatures, nfeatures_plot_ugt2.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\" Without using stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\"Without using custom stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_result_bg3 = nfeature_accuracy_checker(ngram_range=(1, 2))\n",
    "feature_result_tg3 = nfeature_accuracy_checker(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures_plot_tgt3 = pd.DataFrame(feature_result_tgt3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bgt3 = pd.DataFrame(feature_result_bgt3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_ugt3= pd.DataFrame(feature_result_ugt3,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "nfeatures_plot_tg3 = pd.DataFrame(feature_result_tg3,columns=['nfeatures','validation_accuracy'])\n",
    "nfeatures_plot_bg3 = pd.DataFrame(feature_result_bg3,columns=['nfeatures','validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt3.nfeatures, nfeatures_plot_tgt3.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(nfeatures_plot_tg3.nfeatures, nfeatures_plot_tg3.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt3.nfeatures, nfeatures_plot_bgt3.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "plt.plot(nfeatures_plot_bg3.nfeatures, nfeatures_plot_bg3.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt3.nfeatures, nfeatures_plot_ugt3.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\"Using stop words N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary2(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    #t0 = time.time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    #train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    #print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy, sentiment_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words = my_stop_words,max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy, model = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy, model = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/Meghna/Desktop/acl2014using/race/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "test_data_raw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(path):\n",
    "    file_path= os.path.join(path, file)\n",
    "    with open(file_path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            dict_line = json.loads(line)\n",
    "            test_data_raw.append(dict_line['description'])\n",
    "            test_labels.append(0)\n",
    "for data in test_data_raw:\n",
    "    test_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_covid_box_plot = [len(t) for t in test_data]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "test_df = pd.DataFrame(columns=['text','label'])\n",
    "test_df['text'] = test_data\n",
    "test_df['label'] = test_labels\n",
    "test_df['pre_clean_len'] = [len(t) for t in test_df.text]\n",
    "test_df[test_df.pre_clean_len > 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(testing_covid_box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "test_data_raw = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    file_path= os.path.join(path, file)\n",
    "    with open(file_path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            dict_line = json.loads(line)\n",
    "            test_data_raw.append(dict_line['description'])\n",
    "            test_labels.append(0)\n",
    "for data in test_data_raw:\n",
    "    test_data.append(tweet_cleaner_updated(data))\n",
    "    \n",
    "    \n",
    "testing_covid_box_plot = [len(t) for t in test_data]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "test_df = pd.DataFrame(columns=['text','label'])\n",
    "test_df['text'] = test_data\n",
    "test_df['label'] = test_labels\n",
    "test_df['pre_clean_len'] = [len(t) for t in test_df.text]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(testing_covid_box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "df=pd.read_csv('/Users/Meghna/Desktop/Mental_Model_Generation.csv')\n",
    "responses=df.iloc[2:,12:]\n",
    "\n",
    "reordered_responses=responses[['Q34','Q58','Q12','Q13','Q14','Q16','Q35','Q18','Q19','Q20','Q21','Q30','Q34','Q17','Q15','Q1','Q39','Q28','Q3','Q4','Q5','Q6','Q9','Q29_1','Q29_2','Q29_3','Q29_4','Q29_5','Q29_6','Q29_7']]\n",
    "\n",
    "free_form_responses=reordered_responses.iloc[:,:12]\n",
    "\n",
    "single_choice_responses=reordered_responses.iloc[:,12:]\n",
    "\n",
    "individual_responses=collections.defaultdict(list)\n",
    "free_form_responses_without_race=free_form_responses.iloc[:,1:]\n",
    "race=free_form_responses.iloc[:,0].tolist()\n",
    "r=-1\n",
    "for index, row in free_form_responses_without_race.iterrows():\n",
    "    r+=1\n",
    "    for i in range(len(row)):\n",
    "        try:\n",
    "            if float(row[i])==row[i]:\n",
    "                #print(row[i])\n",
    "                continue\n",
    "        except ValueError:\n",
    "            individual_responses[(index,race[r])].extend(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = []\n",
    "survey_labels = []\n",
    "for i in individual_responses:\n",
    "    #print(i[0])\n",
    "    for z in individual_responses[i]:\n",
    "        if type(z) != float:\n",
    "            test_data.append(tweet_cleaner_updated(z))\n",
    "            survey_data.append(tweet_cleaner_updated(z))\n",
    "            test_labels.append(1)\n",
    "            survey_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus, test_labels = test_data, test_labels\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_corpus,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = 'tfidf'   # set 'count' or 'tfidf'\n",
    "analyzer = 'word'  # set 'word' or 'both' ( word and char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vectorizer == 'count':\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = CountVectorizer(stop_words = my_stop_words,ngram_range=(1, 2),max_features=40000)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = CountVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "else:\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english',ngram_range=(1, 2),max_features=40000)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_and_labels(train_corpus, train_labels):    \n",
    "    if analyzer == 'word':\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        \n",
    "        return ngram_vectorized_data, train_labels\n",
    "    else:\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        char_vectorized_data = char_vectorizer.fit_transform(train_corpus)\n",
    "        l = np.hstack((ngram_vectorized_data.toarray(), char_vectorized_data.toarray()))\n",
    "        train_vectorized_data = sparse.csr_matrix(l)\n",
    "        \n",
    "        return train_vectorized_data, train_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_and_labels(test_corpus, test_labels):   \n",
    "    if analyzer == 'word':\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        \n",
    "        return test_ngram_vectorized_data, test_labels\n",
    "    else:\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        test_char_vectorized_data = char_vectorizer.transform(test_corpus)\n",
    "        l2 = np.hstack((test_ngram_vectorized_data.toarray(), test_char_vectorized_data.toarray()))\n",
    "        test_vectorized_data = sparse.csr_matrix(l2)\n",
    "        \n",
    "        return test_vectorized_data,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "x_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary2(pipeline, x_train, y_train, x_val,y_val,x_test, y_test):\n",
    "    if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "    #t0 = time.time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    y_pred = sentiment_fit.predict(x_val)\n",
    "    #train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    \n",
    "    y_true, y_pred_test = y_test, sentiment_fit.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred_test))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test, y_test)\n",
    "    model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words = my_stop_words,max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "eclf3 = MultinomialNB()\n",
    "\n",
    "for clf, label in zip([eclf3], ['Multinomial NB']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words = my_stop_words,max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf4 = MultinomialNB()\n",
    "\n",
    "for clf, label in zip([eclf4], ['Multinomial NB']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncovid_files_path2 = r'noncovid_tweets_aug'\n",
    "noncovid_data2 = []\n",
    "noncovid_labels2 = []\n",
    "\n",
    "with gzip.open(noncovid_files_path2,'r') as fin:\n",
    "    for line in fin:\n",
    "        tweet = json.loads(line)\n",
    "        noncovid_data2.append(tweet_cleaner_updated(tweet['text']))\n",
    "        noncovid_labels2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = survey_data + noncovid_data2\n",
    "y_test2 = survey_labels + noncovid_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf5 = MultinomialNB()\n",
    "\n",
    "for clf, label in zip([eclf5], ['Multinomial NB']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words = my_stop_words,max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf6 = MultinomialNB()\n",
    "\n",
    "for clf, label in zip([eclf6], ['Multinomial NB']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf7 = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf7], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words = my_stop_words,max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf8 = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "    clf_accuracy = accuracy_summary2(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(columns =['train_text'])\n",
    "x_train_df['train_text'] = x_train\n",
    "\n",
    "x_val_df = pd.DataFrame(columns =['val_text'])\n",
    "x_val_df['val_text'] = x_validation\n",
    "\n",
    "x_test_df = pd.DataFrame(columns =['test_text'])\n",
    "x_test_df['test_text'] = x_test\n",
    "\n",
    "x_test2_df = pd.DataFrame(columns =['test2_text'])\n",
    "x_test2_df['test2_text'] = x_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "x_train1 = pd.Series(x_train)\n",
    "x_val1 = pd.Series(x_validation)\n",
    "x_test_s1 = pd.Series(x_test)\n",
    "x_test_s2 = pd.Series(x_test2)\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "all_x = pd.concat([x_train1,x_val1,x_test_s1, x_test_s2])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_x_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, min_count=2, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.dv[prefix]\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train1, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train1, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train1, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_train1, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train1, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [t.split() for t in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_bg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(bigram[t.split()], [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_w2v_bg = labelize_tweets_bg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dbow.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dbow.alpha -= 0.002\n",
    "    model_bg_dbow.min_alpha = model_bg_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_bg = get_vectors(model_bg_dbow, x_train1, 100)\n",
    "validation_vecs_dbow_bg = get_vectors(model_bg_dbow, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dmc = Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dmc.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dmc.alpha -= 0.002\n",
    "    model_bg_dmc.min_alpha = model_bg_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmc_bg = get_vectors(model_bg_dmc, x_train1, 100)\n",
    "validation_vecs_dmc_bg = get_vectors(model_bg_dmc, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmc_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dmm.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dmm.alpha -= 0.002\n",
    "    model_bg_dmm.min_alpha = model_bg_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmm_bg = get_vectors(model_bg_dmm, x_train1, 100)\n",
    "validation_vecs_dmm_bg = get_vectors(model_bg_dmm, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmm_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc_bg = get_concat_vectors(model_bg_dbow,model_bg_dmc, x_train1, 200)\n",
    "validation_vecs_dbow_dmc_bg = get_concat_vectors(model_bg_dbow,model_bg_dmc, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmc_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm_bg = get_concat_vectors(model_bg_dbow,model_bg_dmm, x_train1, 200)\n",
    "validation_vecs_dbow_dmm_bg = get_concat_vectors(model_bg_dbow,model_bg_dmm, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmm_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tg_phrases = Phrases(bigram[tokenized_train])\n",
    "trigram = Phraser(tg_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_tg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(trigram[bigram[t.split()]], [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_w2v_tg = labelize_tweets_tg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tg_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dbow.build_vocab([x for x in tqdm(all_x_w2v_tg)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dbow.alpha -= 0.002\n",
    "    model_tg_dbow.min_alpha = model_tg_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_tg = get_vectors(model_tg_dbow, x_train1, 100)\n",
    "validation_vecs_dbow_tg = get_vectors(model_tg_dbow, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_tg_dmc = Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dmc.build_vocab([x for x in tqdm(all_x_w2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dmc.alpha -= 0.002\n",
    "    model_tg_dmc.min_alpha = model_tg_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmc_tg = get_vectors(model_tg_dmc, x_train1, 100)\n",
    "validation_vecs_dmc_tg = get_vectors(model_tg_dmc, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc_tg, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmc_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_tg_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dmm.build_vocab([x for x in tqdm(all_x_w2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dmm.alpha -= 0.002\n",
    "    model_tg_dmc.min_alpha = model_tg_dmc.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmm_tg = get_vectors(model_tg_dmm, x_train1, 100)\n",
    "validation_vecs_dmm_tg = get_vectors(model_tg_dmm, x_val1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmm_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc_tg = get_concat_vectors(model_tg_dbow,model_tg_dmc, x_train1, 200)\n",
    "validation_vecs_dbow_dmc_tg = get_concat_vectors(model_tg_dbow,model_tg_dmc, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmc_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm_tg = get_concat_vectors(model_tg_dbow,model_tg_dmm, x_train1, 200)\n",
    "validation_vecs_dbow_dmm_tg = get_concat_vectors(model_tg_dbow,model_tg_dmm, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmm_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_train1, 200)\n",
    "validation_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_val1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_ugdbow_tgdmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_ugdbow_tgdmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pp =[]\n",
    "qq =[]\n",
    "tvec = TfidfVectorizer(stop_words = 'english',max_features=40000,ngram_range=(1, 2))\n",
    "x_train_tfidf = tvec.fit_transform(x_train)\n",
    "x_validation_tfidf = tvec.transform(x_validation)\n",
    "chi2score = chi2(x_train_tfidf, y_train)[0]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "wscores = zip(tvec.get_feature_names(), chi2score)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "topchi2 = zip(*wchi2[-20:])\n",
    "for (p,q) in wchi2[-20:]:\n",
    "    pp.append(p)\n",
    "    qq.append(q)\n",
    "x = range(len(qq))\n",
    "labels = pp\n",
    "plt.barh(x,qq, align='center', alpha=0.2)\n",
    "plt.plot(qq, x, '-o', markersize=5, alpha=0.8)\n",
    "plt.yticks(x, labels)\n",
    "plt.xlabel('$\\chi^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "pp =[]\n",
    "qq =[]\n",
    "tvec = CountVectorizer(stop_words = my_stop_words,max_features=48000,ngram_range=(1, 2))\n",
    "x_train_tfidf = tvec.fit_transform(x_train)\n",
    "x_validation_tfidf = tvec.transform(x_validation)\n",
    "chi2score = chi2(x_train_tfidf, y_train)[0]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "wscores = zip(tvec.get_feature_names(), chi2score)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "topchi2 = zip(*wchi2[-20:])\n",
    "for (p,q) in wchi2[-20:]:\n",
    "    pp.append(p)\n",
    "    qq.append(q)\n",
    "x = range(len(qq))\n",
    "labels = pp\n",
    "plt.barh(x,qq, align='center', alpha=0.2)\n",
    "plt.plot(qq, x, '-o', markersize=5, alpha=0.8)\n",
    "plt.yticks(x, labels)\n",
    "plt.xlabel('$\\chi^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "ch2_result = []\n",
    "for n in np.arange(10000,48000,10000):\n",
    "    ch2 = SelectKBest(chi2, k=n)\n",
    "    x_train_chi2_selected = ch2.fit_transform(x_train_tfidf, y_train)\n",
    "    x_validation_chi2_selected = ch2.transform(x_validation_tfidf)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train_chi2_selected, y_train)\n",
    "    score = clf.score(x_validation_chi2_selected, y_validation)\n",
    "    ch2_result.append(score)\n",
    "    print (\"chi2 feature selection evaluation calculated for {} features\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "cvec = TfidfVectorizer()\n",
    "lr = MultinomialNB()\n",
    "n_features = np.arange(10000,48001,10000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Validation result for {} features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_result_bgtmn = nfeature_accuracy_checker(stop_words = 'english',ngram_range=(1, 2))\n",
    "nfeatures_plot_bgtmn = pd.DataFrame(feature_result_bgtmn,columns=['nfeatures','validation_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_bgtmn.nfeatures, nfeatures_plot_bgtmn.validation_accuracy,label='bigram tfidf vectorizer',color='royalblue')\n",
    "plt.plot(np.arange(10000,48000,10000), ch2_result,label='tfidf dimesions reduced from 40,000 features',linestyle=':', color='orangered')\n",
    "\n",
    "plt.title(\"tfidft vectorizer: features limited within tfidft vectorizer VS reduced dimensions with chi2\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "d2v_ugdbow_tgdmm_std = scaler.fit_transform(x_train)\n",
    "d2v_ugdbow_tgdmm_std_val = scaler.fit_transform(x_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "d2v_pca = PCA().fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "x_values = range(1, d2v_pca.n_components_+1)\n",
    "\n",
    "ax.plot(x_values, d2v_pca.explained_variance_ratio_, lw=2, label='explained variance')\n",
    "ax.plot(x_values, np.cumsum(d2v_pca.explained_variance_ratio_), lw=2, label='cumulative explained variance')\n",
    "ax.set_title('Doc2vec (unigram DBOW + trigram DMM) : explained variance of components')\n",
    "ax.set_xlabel('principal component')\n",
    "ax.set_ylabel('explained variance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
