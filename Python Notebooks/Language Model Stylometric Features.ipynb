{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "df=pd.read_csv('tweets_for_language_model')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df['races'].to_list()\n",
    "count = 0\n",
    "for val in races:\n",
    "    if val == 1:\n",
    "        count += 1\n",
    "print(count)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+' #remove mentions\n",
    "pat2 = r'https?://[^ ]+' #remove hyperlinks\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml') # remove html tags\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") #remove byte order marks\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    lower_case = emoji.demojize(lower_case, language='en')\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled) #remove hashtag\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweets'].to_list()\n",
    "cleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    cleaned_tweets.append(tweet_cleaner_updated(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = []\n",
    "for tweet in cleaned_tweets:\n",
    "    length = 0\n",
    "    for word in tweet.split():\n",
    "        if word != '':\n",
    "            length += 1\n",
    "    num_of_words.append(length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_length = []\n",
    "for tweet in cleaned_tweets:\n",
    "    if tweet != '':\n",
    "        lengths = []\n",
    "        for word in tweet.split():\n",
    "            if word != \" \":\n",
    "                lengths.append(len(word))\n",
    "        avg_word_length.append(sum(lengths)/len(lengths))\n",
    "    else:\n",
    "        avg_word_length.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['races'] = races\n",
    "new_df['tweets'] = cleaned_tweets\n",
    "new_df['num_of_words'] = num_of_words\n",
    "new_df['avg_word_length'] = avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = new_df[new_df['races'] == 0]\n",
    "positive_tweets = new_df[new_df['races'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets_num_of_words = negative_tweets['num_of_words'].to_list()\n",
    "pos_tweets_num_of_words = positive_tweets['num_of_words'].to_list()\n",
    "len(neg_tweets_num_of_words)\n",
    "x_pos = [i for i in range(18602)]\n",
    "x_neg = [i for i in range(18602)]\n",
    "plt.scatter(x_pos, pos_tweets_num_of_words,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(x_neg, neg_tweets_num_of_words, marker='o',  color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets_avg_word_len = negative_tweets['avg_word_length'].to_list()\n",
    "pos_tweets_avg_word_len = positive_tweets['avg_word_length'].to_list()\n",
    "len(neg_tweets_num_of_words)\n",
    "x_pos = [i for i in range(18602)]\n",
    "x_neg = [i for i in range(18602)]\n",
    "plt.scatter(x_pos, pos_tweets_avg_word_len,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(x_neg, neg_tweets_avg_word_len, marker='o', color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_words = pd.DataFrame()\n",
    "negs = negative_tweets['num_of_words'].to_list()\n",
    "pos = positive_tweets['num_of_words'].to_list()\n",
    "df_num_words['neg_num_of_words'] = negs\n",
    "df_num_words['pos_num_of_words'] = pos\n",
    "df_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_num_of_words\", y=\"pos_num_of_words\",fit_reg=False, scatter_kws={'alpha':0.5}, data=df_num_words)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_wordlen = pd.DataFrame()\n",
    "negs = negative_tweets['avg_word_length'].to_list()\n",
    "pos = positive_tweets['avg_word_length'].to_list()\n",
    "df_avg_wordlen['neg_avg_word_length'] = negs\n",
    "df_avg_wordlen['pos_avg_word_length'] = pos\n",
    "df_avg_wordlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_avg_word_length\", y=\"pos_avg_word_length\",fit_reg=False, scatter_kws={'alpha':0.5}, data=df_avg_wordlen)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+' #remove mentions\n",
    "pat2 = r'https?://[^ ]+' #remove hyperlinks\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "\n",
    "\n",
    "def tweet_cleaner_updated2(text):\n",
    "    soup = BeautifulSoup(text, 'lxml') # remove html tags\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") #remove byte order marks\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    uncleaned_tweets.append(tweet_cleaner_updated2(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_tweets[3]\n",
    "print(re.split(\" |\\n\",uncleaned_tweets[3])[0] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertools.emoji import extract_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncleaned_num_of_words = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    length = 0\n",
    "    for word in re.split(\" |\\n\",tweet):\n",
    "        if word != '':\n",
    "            length += 1\n",
    "    uncleaned_num_of_words.append(length)\n",
    "    \n",
    "uncleaned_avg_word_length = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    if tweet != '':\n",
    "        lengths = []\n",
    "        for word in re.split(\" |\\n\",tweet):\n",
    "            if word != \" \":\n",
    "                lengths.append(len(word))\n",
    "        uncleaned_avg_word_length.append(sum(lengths)/len(lengths))\n",
    "    else:\n",
    "        uncleaned_avg_word_length.append(0)\n",
    "        \n",
    "        \n",
    "uncleaned_count_emoji = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    count = 0\n",
    "    for i in range(len(extract_emoji(tweet)['emoji_flat_text'])):\n",
    "        count += 1\n",
    "    uncleaned_count_emoji.append(count)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punctuation = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "uncleaned_count_puncts = []\n",
    "for tweet in uncleaned_tweets:\n",
    "    uncleaned_count_puncts.append(count_punctuation(tweet,set(string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncleaned_new_df = pd.DataFrame()\n",
    "uncleaned_new_df['races'] = races\n",
    "uncleaned_new_df['tweets'] = uncleaned_tweets\n",
    "uncleaned_new_df['num_of_words'] = uncleaned_num_of_words\n",
    "uncleaned_new_df['avg_word_length'] = uncleaned_avg_word_length\n",
    "uncleaned_new_df['count_emoji'] = uncleaned_count_emoji\n",
    "uncleaned_new_df['count_puncts'] = uncleaned_count_puncts\n",
    "\n",
    "uncleaned_negative_tweets = uncleaned_new_df[uncleaned_new_df['races'] == 0]\n",
    "uncleaned_positive_tweets = uncleaned_new_df[uncleaned_new_df['races'] == 1]\n",
    "\n",
    "uncleaned_neg_tweets_num_of_words = uncleaned_negative_tweets['num_of_words'].to_list()\n",
    "uncleaned_pos_tweets_num_of_words = uncleaned_positive_tweets['num_of_words'].to_list()\n",
    "len(uncleaned_neg_tweets_num_of_words)\n",
    "uncleaned_x_pos = [i for i in range(18602)]\n",
    "uncleaned_x_neg = [i for i in range(18602)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos_tweets_num_of_words,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_neg_tweets_num_of_words, marker='o',  color = 'green')\n",
    "plt.show()\n",
    "\n",
    "uncleaned_neg_tweets_avg_word_len = uncleaned_negative_tweets['avg_word_length'].to_list()\n",
    "uncleaned_pos_tweets_avg_word_len = uncleaned_positive_tweets['avg_word_length'].to_list()\n",
    "len(uncleaned_neg_tweets_num_of_words)\n",
    "uncleaned_x_pos = [i for i in range(18602)]\n",
    "uncleaned_x_neg = [i for i in range(18602)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos_tweets_avg_word_len,  marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_neg_tweets_avg_word_len, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "uncleaned_df_emoji= pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['count_emoji'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['count_emoji'].to_list()\n",
    "uncleaned_df_emoji['neg_count_emoji'] = uncleaned_negs\n",
    "uncleaned_df_emoji['pos_count_emoji'] = uncleaned_pos\n",
    "uncleaned_df_emoji\n",
    "\n",
    "\n",
    "uncleaned_x_pos = [i for i in range(18602)]\n",
    "uncleaned_x_neg = [i for i in range(18602)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos,marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_negs, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "uncleaned_df_num_words = pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['num_of_words'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['num_of_words'].to_list()\n",
    "uncleaned_df_num_words['neg_num_of_words'] = uncleaned_negs\n",
    "uncleaned_df_num_words['pos_num_of_words'] = uncleaned_pos\n",
    "uncleaned_df_num_words\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_num_of_words\", y=\"pos_num_of_words\",fit_reg=False, scatter_kws={'alpha':0.5}, data=uncleaned_df_num_words)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')\n",
    "\n",
    "\n",
    "uncleaned_df_avg_wordlen = pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['avg_word_length'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['avg_word_length'].to_list()\n",
    "uncleaned_df_avg_wordlen['neg_avg_word_length'] = uncleaned_negs\n",
    "uncleaned_df_avg_wordlen['pos_avg_word_length'] = uncleaned_pos\n",
    "uncleaned_df_avg_wordlen\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_avg_word_length\", y=\"pos_avg_word_length\",fit_reg=False, scatter_kws={'alpha':0.5}, data=uncleaned_df_avg_wordlen)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_count_emoji\", y=\"pos_count_emoji\",fit_reg=False, scatter_kws={'alpha':0.5}, data=uncleaned_df_emoji)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_df_puncts= pd.DataFrame()\n",
    "uncleaned_negs = uncleaned_negative_tweets['count_puncts'].to_list()\n",
    "uncleaned_pos = uncleaned_positive_tweets['count_puncts'].to_list()\n",
    "uncleaned_df_puncts['neg_count_puncts'] = uncleaned_negs\n",
    "uncleaned_df_puncts['pos_count_puncts'] = uncleaned_pos\n",
    "\n",
    "uncleaned_x_pos = [i for i in range(18602)]\n",
    "uncleaned_x_neg = [i for i in range(18602)]\n",
    "plt.scatter(uncleaned_x_pos, uncleaned_pos,marker='^', color = 'purple')\n",
    "plt.show()\n",
    "plt.scatter(uncleaned_x_neg, uncleaned_negs, marker='o', color = 'green')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.regplot(x=\"neg_count_puncts\", y=\"pos_count_puncts\",fit_reg=False, scatter_kws={'alpha':0.5}, data=uncleaned_df_puncts)\n",
    "plt.ylabel('Positive Frequency')\n",
    "plt.xlabel('Negative Frequency')\n",
    "plt.title('Negative Frequency vs Positive Frequency')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_count_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = uncleaned_tweets[989]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punctuation(s,set(string.punctuation))     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
